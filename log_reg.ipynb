{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt # For general plotting\n",
    "from matplotlib import cm\n",
    "\n",
    "from math import ceil, floor\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "plt.rc('font', size=22)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=18)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=16)    # legend fontsize\n",
    "plt.rc('figure', titlesize=22)   # fontsize of the figure title\n",
    "\n",
    "\n",
    "# Set seed to generate reproducible \"pseudo-randomness\" (handles scipy's \"randomness\" too)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Number of training input samples for experiments\n",
    "N_train = [10, 100, 1000]\n",
    "# Number of validation samples for experiments\n",
    "N_valid = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['hispanic_latino',\n 'white',\n 'black',\n 'native_american',\n 'asian',\n 'nhpi',\n 'other_race',\n 'two_or_more',\n 'income',\n 'foreign_born',\n 'gender_ratio',\n 'bachelors',\n 'median_age']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_df = (\n",
    "    pd.read_csv(\"demographics.csv\")\n",
    "    .assign(gender_ratio=lambda x: x.male / x.female)\n",
    "    .drop(columns=[\"male\", \"female\"])\n",
    ")\n",
    "features = [\n",
    "    \"hispanic_latino\",\n",
    "    \"white\",\n",
    "    \"black\",\n",
    "    \"native_american\",\n",
    "    \"asian\",\n",
    "    \"nhpi\",\n",
    "    \"other_race\",\n",
    "    \"two_or_more\",\n",
    "    \"income\",\n",
    "    \"foreign_born\",\n",
    "    \"gender_ratio\",\n",
    "    \"bachelors\",\n",
    "    \"median_age\",\n",
    "]\n",
    "X = demographics_df[features]\n",
    "y = demographics_df[\"margin\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m X_standardized \u001B[38;5;241m=\u001B[39m StandardScaler()\u001B[38;5;241m.\u001B[39mfit_transform(X)\n\u001B[0;32m      2\u001B[0m poly \u001B[38;5;241m=\u001B[39m PolynomialFeatures(\u001B[38;5;28mlen\u001B[39m(features))\n\u001B[1;32m----> 3\u001B[0m X_poly \u001B[38;5;241m=\u001B[39m \u001B[43mpoly\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_standardized\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X_standardized, y, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\base.py:867\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    863\u001B[0m \u001B[38;5;66;03m# non-optimized default implementation; override when a better\u001B[39;00m\n\u001B[0;32m    864\u001B[0m \u001B[38;5;66;03m# method is possible for a given clustering algorithm\u001B[39;00m\n\u001B[0;32m    865\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    866\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[1;32m--> 867\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_polynomial.py:479\u001B[0m, in \u001B[0;36mPolynomialFeatures.transform\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    476\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    477\u001B[0m     \u001B[38;5;66;03m# XP[:, start:end] are terms of degree d - 1\u001B[39;00m\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;66;03m# that exclude feature #feature_idx.\u001B[39;00m\n\u001B[1;32m--> 479\u001B[0m     \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmultiply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mXP\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m:\u001B[49m\u001B[43mend\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_idx\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_idx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mXP\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcurrent_col\u001B[49m\u001B[43m:\u001B[49m\u001B[43mnext_col\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcasting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mno\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    485\u001B[0m     current_col \u001B[38;5;241m=\u001B[39m next_col\n\u001B[0;32m    487\u001B[0m new_index\u001B[38;5;241m.\u001B[39mappend(current_col)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "# The poly transform kind of breaks the notebook at the moment (refrain from running for now )\n",
    "poly = PolynomialFeatures(len(features))\n",
    "X_poly = poly.fit_transform(X_standardized)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Define the logistic/sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the prediction function y = 1 / (1 + np.exp(-X*w))\n",
    "# X.dot(w) inputs to the sigmoid referred to as logits\n",
    "def predict_prob(X, w):\n",
    "    logits = X.dot(w)\n",
    "    return sigmoid(logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NOTE: This implementation may encounter numerical stability issues...\n",
    "# Read into the log-sum-exp trick OR use a class like: sklearn.linear_model import LogisticRegression\n",
    "def log_reg_loss(w, X, y):\n",
    "    # Size of batch\n",
    "    B = X.shape[0]\n",
    "\n",
    "    # Logistic regression model g(X * w)\n",
    "    predictions = predict_prob(X, w)\n",
    "\n",
    "    # NLL loss, 1/N sum [y*log(g(X*w)) + (1-y)*log(1-g(X*w))]\n",
    "    error = predictions - y\n",
    "    nll = -np.mean(y*np.log(predictions) + (1 - y)*np.log(1 - predictions))\n",
    "\n",
    "    # Partial derivative for GD\n",
    "    gradient = (1 / B) * X.T.dot(error)\n",
    "\n",
    "    # Logistic regression loss, NLL (binary cross entropy is another interpretation)\n",
    "    return nll, gradient\n",
    "\n",
    "\n",
    "# Options for mini-batch gradient descent\n",
    "opts = {}\n",
    "opts['max_epoch'] = 100\n",
    "opts['alpha'] = 0.05\n",
    "opts['tolerance'] = 1e-3\n",
    "\n",
    "opts['batch_size'] = 10\n",
    "\n",
    "# Breaks the matrix X and vector y into batches\n",
    "def batchify(X, y, batch_size, N):\n",
    "    shuffled_indices = np.random.permutation(N) # Returns a permutation of indices up to N\n",
    "\n",
    "    # Shuffle row-wise X (i.e. across training examples) and labels using same permuted order\n",
    "    X = X[shuffled_indices]\n",
    "    y = y[shuffled_indices]\n",
    "\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Iterate over N in batch_size steps, last batch may be < batch_size\n",
    "    for i in range(0, N, batch_size):\n",
    "        nxt = min(i + batch_size, N + 1)\n",
    "        X_batch.append(X[i:nxt, :])\n",
    "        y_batch.append(y[i:nxt])\n",
    "\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "def gradient_descent(loss_func, theta0, X, y, N, *args, **kwargs):\n",
    "    # Mini-batch GD. Stochastic GD if batch_size=1.\n",
    "\n",
    "    # Break up data into batches and work out gradient for each batch\n",
    "    # Move parameters theta in that direction, scaled by the step size.\n",
    "\n",
    "    # Options for total sweeps over data (max_epochs),\n",
    "    # and parameters, like learning rate and threshold.\n",
    "\n",
    "    # Default options\n",
    "    max_epoch = kwargs['max_epoch'] if 'max_epoch' in kwargs else 200\n",
    "    alpha = kwargs['alpha'] if 'alpha' in kwargs else 0.1\n",
    "    epsilon = kwargs['tolerance'] if 'tolerance' in kwargs else 1e-6\n",
    "\n",
    "    batch_size = kwargs['batch_size'] if 'batch_size' in kwargs else 10\n",
    "\n",
    "    # Turn the data into batches\n",
    "    X_batch, y_batch = batchify(X, y, batch_size, N)\n",
    "    num_batches = len(y_batch)\n",
    "    print(\"%d batches of size %d\\n\" % (num_batches, batch_size))\n",
    "\n",
    "    theta = theta0\n",
    "    m_t = np.zeros(theta.shape)\n",
    "\n",
    "    trace = {}\n",
    "    trace['loss'] = []\n",
    "    trace['theta'] = []\n",
    "\n",
    "    # Main loop:\n",
    "    for epoch in range(1, max_epoch + 1):\n",
    "        # print(\"epoch %d\\n\" % epoch)\n",
    "\n",
    "        loss_epoch = 0\n",
    "        for b in range(num_batches):\n",
    "            X_b = X_batch[b]\n",
    "            y_b = y_batch[b]\n",
    "            # print(\"epoch %d batch %d\\n\" % (epoch, b))\n",
    "\n",
    "            # Compute NLL loss and gradient of NLL function\n",
    "            loss, gradient = loss_func(theta, X_b, y_b, *args)\n",
    "            loss_epoch += loss\n",
    "\n",
    "            # Steepest descent update\n",
    "            theta = theta - alpha * gradient\n",
    "\n",
    "            # Terminating Condition is based on how close we are to minimum (gradient = 0)\n",
    "            if np.linalg.norm(gradient) < epsilon:\n",
    "                print(\"Gradient Descent has converged after {} epochs\".format(epoch))\n",
    "                break\n",
    "\n",
    "        # Storing the history of the parameters and loss values per epoch\n",
    "        trace['loss'].append(np.mean(loss_epoch))\n",
    "        trace['theta'].append(theta)\n",
    "\n",
    "        # Also break epochs loop\n",
    "        if np.linalg.norm(gradient) < epsilon:\n",
    "            break\n",
    "\n",
    "    return theta, trace"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}